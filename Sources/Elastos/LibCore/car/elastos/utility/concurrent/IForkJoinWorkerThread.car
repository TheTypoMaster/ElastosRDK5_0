
module
{
    interface Elastos.Utility.ICollection;

    namespace Elastos {
    namespace Utility {
    namespace Concurrent {
    /**
     * A thread managed by a {@link ForkJoinPool}, which executes
     * {@link ForkJoinTask}s.
     * This class is subclassable solely for the sake of adding
     * functionality -- there are no overridable methods dealing with
     * scheduling or execution.  However, you can override initialization
     * and termination methods surrounding the main task processing loop.
     * If you do create such a subclass, you will also need to supply a
     * custom {@link ForkJoinPool.ForkJoinWorkerThreadFactory} to use it
     * in a {@code ForkJoinPool}.
     *
     * @since 1.7
     * @hide
     * @author Doug Lea
     */

    /**
     * @Involve
     * interface ISynchronize;
     * interface IThread;
     */
    interface IForkJoinWorkerThread {
        /*
         * Overview:
         *
         * ForkJoinWorkerThreads are managed by ForkJoinPools and perform
         * ForkJoinTasks. This class includes bookkeeping in support of
         * worker activation, suspension, and lifecycle control described
         * in more detail in the internal documentation of class
         * ForkJoinPool. And as described further below, this class also
         * includes special-cased support for some ForkJoinTask
         * methods. But the main mechanics involve work-stealing:
         *
         * Work-stealing queues are special forms of Deques that support
         * only three of the four possible end-operations -- push, pop,
         * and deq (aka steal), under the further constraints that push
         * and pop are called only from the owning thread, while deq may
         * be called from other threads.  (If you are unfamiliar with
         * them, you probably want to read Herlihy and Shavit's book "The
         * Art of Multiprocessor programming", chapter 16 describing these
         * in more detail before proceeding.)  The main work-stealing
         * queue design is roughly similar to those in the papers "Dynamic
         * Circular Work-Stealing Deque" by Chase and Lev, SPAA 2005
         * (http://research.sun.com/scalable/pubs/index.html) and
         * "Idempotent work stealing" by Michael, Saraswat, and Vechev,
         * PPoPP 2009 (http://portal.acm.org/citation.cfm?id=1504186).
         * The main differences ultimately stem from gc requirements that
         * we null out taken slots as soon as we can, to maintain as small
         * a footprint as possible even in programs generating huge
         * numbers of tasks. To accomplish this, we shift the CAS
         * arbitrating pop vs deq (steal) from being on the indices
         * ("queueBase" and "queueTop") to the slots themselves (mainly
         * via method "casSlotNull()"). So, both a successful pop and deq
         * mainly entail a CAS of a slot from non-null to null.  Because
         * we rely on CASes of references, we do not need tag bits on
         * queueBase or queueTop.  They are simple ints as used in any
         * circular array-based queue (see for example ArrayDeque).
         * Updates to the indices must still be ordered in a way that
         * guarantees that queueTop == queueBase means the queue is empty,
         * but otherwise may err on the side of possibly making the queue
         * appear nonempty when a push, pop, or deq have not fully
         * committed. Note that this means that the deq operation,
         * considered individually, is not wait-free. One thief cannot
         * successfully continue until another in-progress one (or, if
         * previously empty, a push) completes.  However, in the
         * aggregate, we ensure at least probabilistic non-blockingness.
         * If an attempted steal fails, a thief always chooses a different
         * random victim target to try next. So, in order for one thief to
         * progress, it suffices for any in-progress deq or new push on
         * any empty queue to complete.
         *
         * This approach also enables support for "async mode" where local
         * task processing is in FIFO, not LIFO order; simply by using a
         * version of deq rather than pop when locallyFifo is true (as set
         * by the ForkJoinPool).  This allows use in message-passing
         * frameworks in which tasks are never joined.  However neither
         * mode considers affinities, loads, cache localities, etc, so
         * rarely provide the best possible performance on a given
         * machine, but portably provide good throughput by averaging over
         * these factors.  (Further, even if we did try to use such
         * information, we do not usually have a basis for exploiting
         * it. For example, some sets of tasks profit from cache
         * affinities, but others are harmed by cache pollution effects.)
         *
         * When a worker would otherwise be blocked waiting to join a
         * task, it first tries a form of linear helping: Each worker
         * records (in field currentSteal) the most recent task it stole
         * from some other worker. Plus, it records (in field currentJoin)
         * the task it is currently actively joining. Method joinTask uses
         * these markers to try to find a worker to help (i.e., steal back
         * a task from and execute it) that could hasten completion of the
         * actively joined task. In essence, the joiner executes a task
         * that would be on its own local deque had the to-be-joined task
         * not been stolen. This may be seen as a conservative variant of
         * the approach in Wagner & Calder "Leapfrogging: a portable
         * technique for implementing efficient futures" SIGPLAN Notices,
         * 1993 (http://portal.acm.org/citation.cfm?id=155354). It differs
         * in that: (1) We only maintain dependency links across workers
         * upon steals, rather than use per-task bookkeeping.  This may
         * require a linear scan of workers array to locate stealers, but
         * usually doesn't because stealers leave hints (that may become
         * stale/wrong) of where to locate them. This isolates cost to
         * when it is needed, rather than adding to per-task overhead.
         * (2) It is "shallow", ignoring nesting and potentially cyclic
         * mutual steals.  (3) It is intentionally racy: field currentJoin
         * is updated only while actively joining, which means that we
         * miss links in the chain during long-lived tasks, GC stalls etc
         * (which is OK since blocking in such cases is usually a good
         * idea).  (4) We bound the number of attempts to find work (see
         * MAX_HELP) and fall back to suspending the worker and if
         * necessary replacing it with another.
         *
         * Efficient implementation of these algorithms currently relies
         * on an uncomfortable amount of "Unsafe" mechanics. To maintain
         * correct orderings, reads and writes of variable queueBase
         * require volatile ordering.  Variable queueTop need not be
         * volatile because non-local reads always follow those of
         * queueBase.  Similarly, because they are protected by volatile
         * queueBase reads, reads of the queue array and its slots by
         * other threads do not need volatile load semantics, but writes
         * (in push) require store order and CASes (in pop and deq)
         * require (volatile) CAS semantics.  (Michael, Saraswat, and
         * Vechev's algorithm has similar properties, but without support
         * for nulling slots.)  Since these combinations aren't supported
         * using ordinary volatiles, the only way to accomplish these
         * efficiently is to use direct Unsafe calls. (Using external
         * AtomicIntegers and AtomicReferenceArrays for the indices and
         * array is significantly slower because of memory locality and
         * indirection effects.)
         *
         * Further, performance on most platforms is very sensitive to
         * placement and sizing of the (resizable) queue array.  Even
         * though these queues don't usually become all that big, the
         * initial size must be large enough to counteract cache
         * contention effects across multiple queues (especially in the
         * presence of GC cardmarking). Also, to improve thread-locality,
         * queues are initialized after starting.
         */

        // Public methods

        /**
         * Returns the pool hosting this thread.
         *
         * @return the pool
         */
        GetPool(
            [out] IForkJoinPool** outpool);

        /**
         * Returns the index number of this thread in its pool.  The
         * returned value ranges from zero to the maximum number of
         * threads (minus one) that have ever been created in the pool.
         * This method may be useful for applications that track status or
         * collect results per-worker rather than per-task.
         *
         * @return the index number
         */
        GetPoolIndex(
            [out] Int32* value);

        /**
         * This method is required to be public, but should never be
         * called explicitly. It performs the main run loop to execute
         * {@link ForkJoinTask}s.
         */
        Run();

        /*
         * Intrinsics-based atomic writes for queue slots. These are
         * basically the same as methods in AtomicReferenceArray, but
         * specialized for (1) ForkJoinTask elements (2) requirement that
         * nullness and bounds checks have already been performed by
         * callers and (3) effective offsets are known not to overflow
         * from int to long (because of MAXIMUM_QUEUE_CAPACITY). We don't
         * need corresponding version for reads: plain array reads are OK
         * because they are protected by other volatile reads and are
         * confirmed by CASes.
         *
         * Most uses don't actually call these methods, but instead
         * contain inlined forms that enable more predictable
         * optimization.  We don't define the version of write used in
         * pushTask at all, but instead inline there a store-fenced array
         * slot write.
         *
         * Also in most methods, as a performance (not correctness) issue,
         * we'd like to encourage compilers not to arbitrarily postpone
         * setting queueTop after writing slot.  Currently there is no
         * intrinsic for arranging this, but using Unsafe putOrderedInt
         * may be a preferable strategy on some compilers even though its
         * main effect is a pre-, not post- fence. To simplify possible
         * changes, the option is left in comments next to the associated
         * assignments.
         */

        // queue methods

        /**
         * Pushes a task. Call only from this thread.
         *
         * @param t the task. Caller must ensure non-null.
         */
        PushTask(
            [in] IForkJoinTask* t);

        /**
         * Tries to take a task from the base of the queue, failing if
         * empty or contended. Note: Specializations of this code appear
         * in locallyDeqTask and elsewhere.
         *
         * @return a task, or null if none or contended
         */
        DeqTask(
            [out] IForkJoinTask** outtask);

        /**
         * Tries to take a task from the base of own queue.  Called only
         * by this thread.
         *
         * @return a task, or null if none
         */
        LocallyDeqTask(
            [out] IForkJoinTask** outtask);

        /**
         * Specialized version of popTask to pop only if topmost element
         * is the given task. Called only by this thread.
         *
         * @param t the task. Caller must ensure non-null.
         */
        UnpushTask(
            [in] IForkJoinTask* t,
            [out] Boolean* value);

        /**
         * Returns next task, or null if empty or contended.
         */
        PeekTask(
            [out] IForkJoinTask** outtask);

        // Support methods for ForkJoinPool

        /**
         * Runs the given task, plus any local tasks until queue is empty
         */
        ExecTask(
            [in] IForkJoinTask* t);

        /**
         * Removes and cancels all tasks in queue.  Can be called from any
         * thread.
         */
        CancelTasks();

        /**
         * Drains tasks to given collection c.
         *
         * @return the number of tasks drained
         */
        DrainTasksTo(
            [in] ICollection* c,
            [out] Int32* value);

        // Support methods for ForkJoinTask

        /**
         * Returns an estimate of the number of tasks in the queue.
         */
        GetQueueSize(
            [out] Int32* value);

        /**
         * Gets and removes a local task.
         *
         * @return a task, if available
         */
        PollLocalTask(
            [out] IForkJoinTask** outtask);

        /**
         * Gets and removes a local or stolen task.
         *
         * @return a task, if available
         */
        PollTask(
            [out] IForkJoinTask** outtask);

        /**
         * Possibly runs some tasks and/or blocks, until joinMe is done.
         *
         * @param joinMe the task to join
         * @return completion status on exit
         */
        JoinTask(
            [in] IForkJoinTask* joinMe,
            [out] Int32* value);

        /**
         * Implements ForkJoinTask.getSurplusQueuedTaskCount().  Returns
         * an estimate of the number of tasks, offset by a function of
         * number of idle workers.
         *
         * This method provides a cheap heuristic guide for task
         * partitioning when programmers, frameworks, tools, or languages
         * have little or no idea about task granularity.  In essence by
         * offering this method, we ask users only about tradeoffs in
         * overhead vs expected throughput and its variance, rather than
         * how finely to partition tasks.
         *
         * In a steady state strict (tree-structured) computation, each
         * thread makes available for stealing enough tasks for other
         * threads to remain active. Inductively, if all threads play by
         * the same rules, each thread should make available only a
         * constant number of tasks.
         *
         * The minimum useful constant is just 1. But using a value of 1
         * would require immediate replenishment upon each steal to
         * maintain enough tasks, which is infeasible.  Further,
         * partitionings/granularities of offered tasks should minimize
         * steal rates, which in general means that threads nearer the top
         * of computation tree should generate more than those nearer the
         * bottom. In perfect steady state, each thread is at
         * approximately the same level of computation tree. However,
         * producing extra tasks amortizes the uncertainty of progress and
         * diffusion assumptions.
         *
         * So, users will want to use values larger, but not much larger
         * than 1 to both smooth over transient shortages and hedge
         * against uneven progress; as traded off against the cost of
         * extra task overhead. We leave the user to pick a threshold
         * value to compare with the results of this call to guide
         * decisions, but recommend values such as 3.
         *
         * When all threads are active, it is on average OK to estimate
         * surplus strictly locally. In steady-state, if one thread is
         * maintaining say 2 surplus tasks, then so are others. So we can
         * just use estimated queue length (although note that (queueTop -
         * queueBase) can be an overestimate because of stealers lagging
         * increments of queueBase).  However, this strategy alone leads
         * to serious mis-estimates in some non-steady-state conditions
         * (ramp-up, ramp-down, other stalls). We can detect many of these
         * by further considering the number of "idle" threads, that are
         * known to have zero queued tasks, so compensate by a factor of
         * (#idle/#active) threads.
         */
        GetEstimatedSurplusTaskCount(
            [out] Int32* value);

        /**
         * Runs tasks until {@code pool.isQuiescent()}. We piggyback on
         * pool's active count ctl maintenance, but rather than blocking
         * when tasks cannot be found, we rescan until all others cannot
         * find tasks either. The bracketing by pool quiescerCounts
         * updates suppresses pool auto-shutdown mechanics that could
         * otherwise prematurely terminate the pool because all threads
         * appear to be inactive.
         */
        HelpQuiescePool();
    }

    } // namespace Concurrent
    } // namespace Utility
    } // namespace Elastos
}